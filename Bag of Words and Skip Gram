{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Bag of Words and Skip Gram","provenance":[],"collapsed_sections":[],"mount_file_id":"1RIYyDv1rrigTosLDdjqlWR3C0VBUI_zh","authorship_tag":"ABX9TyOAkio39bsKliEMMRHP5DeA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"nSGc3dy6azXI"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","import pandas as pd\n","from torchvision import transforms\n","\n","torch.manual_seed(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uYL34M-3bEYS"},"source":["class EmojiDataset(Dataset):\n","    def __init__(self, csv_file, transform=None):\n","        self.tweets_frame = pd.read_csv(csv_file)\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.tweets_frame)\n","\n","    def __getitem__(self, index):\n","        if torch.is_tensor(index):\n","            index = index.tolist()\n","\n","        sentence = str(self.tweets_frame.iloc[index, 0])\n","        words = np.array(sentence.split(\" \"))\n","        emoji = self.tweets_frame.iloc[index, 1]\n","\n","        return emoji, \" \".join(words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SN2O7AepbGn2"},"source":["from collections import defaultdict\n","\n","emojis_we_want = {':red_heart:': 0, ':face_with_tears_of_joy:': 0, ':loudly_crying_face:': 0, ':smiling_face_with_heart-eyes:': 0, ':fire:': 0}\n","training_data = EmojiDataset(csv_file='/content/drive/MyDrive/Colab Notebooks/py_dev_clean.csv')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_pFliVyubKPq"},"source":["print(len(training_data))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-GN_EG_gbZPQ"},"source":["from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","\n","tokenizer = get_tokenizer('basic_english')\n","\n","def yield_tokens(data_iter):\n","    for _, text in data_iter:\n","        yield tokenizer(text)\n","\n","vocab = build_vocab_from_iterator(yield_tokens(training_data), specials=[\"<unk>\"])\n","vocab.set_default_index(vocab[\"<unk>\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KtaNVlqqdyRX"},"source":["vocab(['here', 'is', 'an', 'example'])\n","# vocab(tokenizer(\"here is an example\"))\n","print(len(vocab))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TgJmHPg5baxr"},"source":["tag_to_ix = {}\n","# For each words-list (sentence) and tags-list in each tuple of training_data\n","for emoji, words in training_data:\n","    if emoji not in tag_to_ix:\n","        tag_to_ix[emoji] = len(tag_to_ix)  # Assign each word with a unique index\n","print(tag_to_ix)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YP_4fkxOeObM"},"source":["text_pipeline = lambda x: vocab(tokenizer(x))\n","label_pipeline = lambda x: tag_to_ix[x]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zems6dSJdKxC"},"source":["from torch.utils.data import DataLoader\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","def collate_batch(batch):\n","    label_list, text_list, offsets = [], [], [0]\n","    for (_label, _text) in batch:\n","         label_list.append(label_pipeline(_label))\n","         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n","         text_list.append(processed_text)\n","         offsets.append(processed_text.size(0))\n","    label_list = torch.tensor(label_list, dtype=torch.int64)\n","    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n","    text_list = torch.cat(text_list)\n","    return label_list.to(device), text_list.to(device), offsets.to(device)\n","\n","dataloader = DataLoader(training_data, batch_size=8, shuffle=False, collate_fn=collate_batch)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vIGIosbxbuuv"},"source":["class TextClassificationModelBOW(nn.Module):\n","\n","    def __init__(self, vocab_size, embed_dim, num_class):\n","        super(TextClassificationModelBOW, self).__init__()\n","        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n","        self.fc = nn.Linear(embed_dim, num_class)\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        initrange = 0.5\n","        self.embedding.weight.data.uniform_(-initrange, initrange)\n","        self.fc.weight.data.uniform_(-initrange, initrange)\n","        self.fc.bias.data.zero_()\n","\n","    def forward(self, text, offsets):\n","        embedded = self.embedding(text, offsets)\n","        return self.fc(embedded)\n","\n","class TextClassificationModelSKIP(nn.Module):\n","\n","    def __init__(self, vocab_size, embed_dim, num_class):\n","        super(TextClassificationModelSKIP, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_dim, sparse=True)\n","        self.fc1 = nn.Linear(embed_dim, 128)\n","        self.fc2 = nn.Linear(128, num_class)\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        initrange = 0.5\n","        self.embedding.weight.data.uniform_(-initrange, initrange)\n","        self.fc1.weight.data.uniform_(-initrange, initrange)\n","        self.fc1.bias.data.zero_()\n","        self.fc2.weight.data.uniform_(-initrange, initrange)\n","        self.fc2.bias.data.zero_()\n","\n","    def forward(self, text):\n","        embedded = self.embedding(text)\n","        x = F.relu(self.fc1(embedded))\n","        x = F.relu(self.fc2(embedded))\n","        return F.log_softmax(x, dim=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DvWDIFlQM0hD"},"source":["def prepare_sequence(seq, to_ix):\n","    idxs = [vocab(w) for w in seq]\n","    return torch.tensor(idxs, dtype=torch.long)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WdKBf_rvMpxc"},"source":["EMBEDDING_DIM = 100\n","emojis = tag_to_ix.keys()\n","emojis_tensor = prepare_sequence(emojis, tag_to_ix)\n","emoji_embeddings = nn.Embedding(len(emojis), EMBEDDING_DIM)\n","\n","emoji_embeds = emoji_embeddings(emojis_tensor)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bKI6S7fhNgZ9"},"source":["from collections import defaultdict\n","\n","emojis_we_want = {\n","    ':red_heart:': 0, \n","    ':face_with_tears_of_joy:': 0, \n","    ':loudly_crying_face:': 0, \n","    ':smiling_face_with_heart-eyes:': 0, \n","    ':fire:': 0,\n","    ':folded_hands:': 0,\n","    ':weary_face:': 0,\n","    ':person_shrugging:': 0,\n","    ':two_hearts:': 0,\n","    ':sparkles:': 0\n","  }\n","training_data = EmojiDataset(csv_file='/content/drive/MyDrive/Colab Notebooks/py_dev_clean.csv')\n","train_sample = []\n","done = 0\n","i = 0\n","\n","num_in_emoji = 30000\n","\n","for emoji, sentence in training_data:\n","    if emoji in emojis_we_want.keys() and emojis_we_want[emoji] < num_in_emoji:\n","        train_sample.append(sample)\n","        emojis_we_want[emoji] += 1\n","\n","    if sum(list(emojis_we_want.values())) >= len(emojis_we_want.keys())*num_in_emoji:\n","      break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h47mxW5JOBhm"},"source":["print(len(train_sample))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NUrnL3VwM3Uu"},"source":["device = 'cuda' if torch.cuda.is_available()==True else 'cpu'\n","device = torch.device(device)\n","print(device)\n","\n","model = TextClassificationModelSKIP(len(vocab), EMBEDDING_DIM, len(tag_to_ix))\n","model = model.to(device)\n","\n","loss_function = nn.NLLLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.01)\n","model.train()\n","\n","\n","for epoch in range(3):\n","    epoch_loss = 0\n","    samples_seen = 0\n","\n","    partial_loss = 0\n","    for tag, sentence in train_sample:\n","        # sentence = sample['words']\n","        # tag = sample['emoji']\n","\n","        model.zero_grad()\n","\n","        # Step 2. Get our inputs ready for the network, that is, turn them into\n","        # Tensors of word indices.\n","\n","        # Step 3. Run our forward pass.\n","        sentence = sentence.to(device)\n","        tag_scores = model(sentence)\n","\n","        # Step 4. Compute the loss, gradients, and update the parameters by\n","        #  calling optimizer.step()\n","\n","        targets = targets.to(device)\n","        loss = loss_function(tag_scores, targets)\n","        epoch_loss += loss.item()\n","        partial_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","        samples_seen += 1\n","        if samples_seen % 1000 == 0:\n","            print(epoch, samples_seen, epoch_loss/samples_seen, partial_loss/1000)\n","            partial_loss = 0\n","\n","        \n","\n","# See what the scores are after training"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ljhGRh2qbyAY"},"source":["device = 'cuda' if torch.cuda.is_available()==True else 'cpu'\n","device = torch.device(device)\n","print(device)\n","\n","num_class = len(tag_to_ix.keys())\n","vocab_size = len(vocab)\n","emsize = 64\n","model = TextClassificationModelBOW(vocab_size, emsize, num_class).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5bPnepl4cp2j"},"source":["import time\n","\n","def train(dataloader):\n","    model.train()\n","    total_acc, total_count = 0, 0\n","    log_interval = 500\n","    start_time = time.time()\n","\n","    for idx, (label, text, offsets) in enumerate(dataloader):\n","        if label == 'words': continue\n","        optimizer.zero_grad()\n","        predicted_label = model(text, offsets)\n","        loss = criterion(predicted_label, label)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n","        optimizer.step()\n","        total_acc += (predicted_label.argmax(1) == label).sum().item()\n","        total_count += label.size(0)\n","        if idx % log_interval == 0 and idx > 0:\n","            elapsed = time.time() - start_time\n","            print('| epoch {:3d} | {:5d}/{:5d} batches '\n","                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n","                                              total_acc/total_count))\n","            total_acc, total_count = 0, 0\n","            start_time = time.time()\n","\n","def evaluate(dataloader):\n","    model.eval()\n","    total_acc, total_count = 0, 0\n","\n","    with torch.no_grad():\n","        for idx, (label, text, offsets) in enumerate(dataloader):\n","            predicted_label = model(text, offsets)\n","            loss = criterion(predicted_label, label)\n","            total_acc += (predicted_label.argmax(1) == label).sum().item()\n","            total_count += label.size(0)\n","    return total_acc/total_count"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rq3EictcesML"},"source":["from torch.utils.data.dataset import random_split\n","from torchtext.data.functional import to_map_style_dataset\n","# Hyperparameters\n","EPOCHS = 10 # epoch\n","LR = 5  # learning rate\n","BATCH_SIZE = 128 # batch size for training\n","\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n","total_accu = None\n","\n","training_data = training_data\n","train_iter, test_iter = training_data, training_data\n","train_dataset = train_iter\n","test_dataset = test_iter\n","num_train = int(len(train_dataset) * 0.95)\n","split_train_, split_valid_ = \\\n","    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n","\n","train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n","                              shuffle=True, collate_fn=collate_batch)\n","valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n","                              shuffle=True, collate_fn=collate_batch)\n","test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n","                             shuffle=True, collate_fn=collate_batch)\n","\n","for epoch in range(1, EPOCHS + 1):\n","    epoch_start_time = time.time()\n","    train(dataloader)\n","    accu_val = evaluate(valid_dataloader)\n","    if total_accu is not None and total_accu > accu_val:\n","      scheduler.step()\n","    else:\n","       total_accu = accu_val\n","    print('-' * 59)\n","    print('| end of epoch {:3d} | time: {:5.2f}s | '\n","          'valid accuracy {:8.3f} '.format(epoch,\n","                                           time.time() - epoch_start_time,\n","                                           accu_val))\n","    print('-' * 59)"],"execution_count":null,"outputs":[]}]}