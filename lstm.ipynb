{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lstm.ipynb","provenance":[],"collapsed_sections":[]},"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"},"kernelspec":{"display_name":"Python 3.6.9 64-bit","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ZOg2QY8WdQn1"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","import pandas as pd\n","from torchvision import transforms\n","\n","torch.manual_seed(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xOyglUuYdQn5"},"source":["class EmojiDataset(Dataset):\n","    def __init__(self, csv_file, transform=None):\n","        self.tweets_frame = pd.read_csv(csv_file)\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.landmarks_frame)\n","\n","    def __getitem__(self, index):\n","        if torch.is_tensor(index):\n","            index = index.tolist()\n","\n","        sentence = str(self.tweets_frame.iloc[index, 0])\n","        words = np.array(sentence.split(\" \"))\n","        emoji = self.tweets_frame.iloc[index, 1]\n","\n","        sample = {'words': words, 'emoji':emoji}\n","        if self.transform:\n","            sample = self.transform(sample)\n","\n","        return sample"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8PgddVAf2j3m"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zNG9oNce7wDu"},"source":["\n","from collections import defaultdict\n","\n","emojis_we_want = {\n","    ':red_heart:': 0, \n","    ':face_with_tears_of_joy:': 0, \n","    ':loudly_crying_face:': 0, \n","    ':smiling_face_with_heart-eyes:': 0, \n","    ':fire:': 0,\n","    ':folded_hands:': 0,\n","    ':weary_face:': 0,\n","    ':person_shrugging:': 0,\n","    ':two_hearts:': 0,\n","    ':sparkles:': 0\n","  }\n","training_data = EmojiDataset(csv_file='/content/drive/MyDrive/Colab Notebooks/py_dev_clean.csv')\n","train_sample = []\n","done = 0\n","i = 0\n","\n","num_in_emoji = 30000\n","\n","for sample in training_data:\n","    if sample['emoji'] in emojis_we_want.keys() and emojis_we_want[sample['emoji']] < num_in_emoji:\n","        train_sample.append(sample)\n","        emojis_we_want[sample['emoji']] += 1\n","\n","    if sum(list(emojis_we_want.values())) >= len(emojis_we_want.keys())*num_in_emoji:\n","      break\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XrgFkZr49aHs"},"source":["import random\n","\n","random.shuffle(train_sample)\n","\n","cut = int(0.2*len(train_sample))\n","test_sample = train_sample[:cut]\n","train_sample = train_sample[cut:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NK1-x2j6Ffgb"},"source":["print(len(train_sample))\n","print(len(test_sample))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZLcklN1d1tUs"},"source":["word_to_ix = {}\n","# For each words-list (sentence) and tags-list in each tuple of training_data\n","for sample in train_sample:\n","    for word in sample['words']:\n","        if word not in word_to_ix:\n","            word_to_ix[word] = len(word_to_ix)  # Assign each word with a unique index\n","\n","for sample in test_sample:\n","    for word in sample['words']:\n","        if word not in word_to_ix:\n","            word_to_ix[word] = len(word_to_ix)  # Assign each word with a unique index\n","print(len(word_to_ix))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IK42KbM0dQn7"},"source":["tag_to_ix = {}\n","# For each words-list (sentence) and tags-list in each tuple of training_data\n","for sample in train_sample:\n","    if sample['emoji'] not in tag_to_ix:\n","        tag_to_ix[sample['emoji']] = len(tag_to_ix)  # Assign each word with a unique index\n","print(tag_to_ix)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WAWWRRnu4p9r"},"source":["def prepare_sequence(seq, to_ix):\n","    idxs = [to_ix[w] for w in seq]\n","    return torch.tensor(idxs, dtype=torch.long)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uDrOaxvadQn-"},"source":["class LSTMTagger(nn.Module):\n","\n","    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n","        super(LSTMTagger, self).__init__()\n","        self.embedding_dim = embedding_dim\n","        self.hidden_dim = hidden_dim\n","\n","        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n","        self.emojis = prepare_sequence(emojis, tag_to_ix)\n","\n","        # The LSTM takes word embeddings as inputs, and outputs hidden states\n","        # with dimensionality hidden_dim.\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n","\n","        # The linear layer that maps from hidden state space to tag space\n","        self.hidden2tag = nn.Linear(hidden_dim*2, tagset_size)\n","\n","    def forward(self, sentence):\n","        embeds = self.word_embeddings(sentence)\n","        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n","        lstm_out = torch.relu(lstm_out)\n","\n","        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n","        tag_scores = F.log_softmax(tag_space, dim=1)\n","        return tag_scores"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ItnKjohTdQn_"},"source":["EMBEDDING_DIM = 100\n","HIDDEN_DIM = 64\n","emojis = tag_to_ix.keys()\n","emojis_tensor = prepare_sequence(emojis, tag_to_ix)\n","emoji_embeddings = nn.Embedding(len(emojis), EMBEDDING_DIM)\n","\n","emoji_embeds = emoji_embeddings(emojis_tensor)\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9bPIy0tMdllW"},"source":["device = 'cuda' if torch.cuda.is_available()==True else 'cpu'\n","device = torch.device(device)\n","print(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h5a0qYyqdQoA"},"source":["device = 'cuda' if torch.cuda.is_available()==True else 'cpu'\n","device = torch.device(device)\n","print(device)\n","\n","model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n","model = model.to(device)\n","\n","loss_function = nn.NLLLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.01)\n","model.train()\n","\n","\n","for epoch in range(3):\n","    epoch_loss = 0\n","    samples_seen = 0\n","\n","    partial_loss = 0\n","    for sample in train_sample:\n","        sentence = sample['words']\n","        tag = sample['emoji']\n","\n","        model.zero_grad()\n","\n","        # Step 2. Get our inputs ready for the network, that is, turn them into\n","        # Tensors of word indices.\n","        sentence_in = prepare_sequence(sentence, word_to_ix)\n","        targets = prepare_sequence([tag]*len(sentence), tag_to_ix)\n","\n","        # Step 3. Run our forward pass.\n","        sentence_in = sentence_in.to(device)\n","        tag_scores = model(sentence_in)\n","\n","        # Step 4. Compute the loss, gradients, and update the parameters by\n","        #  calling optimizer.step()\n","\n","        targets = targets.to(device)\n","        loss = loss_function(tag_scores, targets)\n","        epoch_loss += loss.item()\n","        partial_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","        samples_seen += 1\n","        if samples_seen % 1000 == 0:\n","            print(epoch, samples_seen, epoch_loss/samples_seen, partial_loss/1000)\n","            partial_loss = 0\n","\n","        \n","\n","# See what the scores are after training"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"neMIUMBtdQoA"},"source":["import scipy.stats as stats\n","import random\n","# {':red_heart:': 0, ':face_with_tears_of_joy:': 1, ':smiling_face_with_heart-eyes:': 2, ':loudly_crying_face:': 3, ':fire:': 4}\n","ix_to_emoji = {v:k for k,v in tag_to_ix.items()}\n","x = 0\n","\n","i = 0\n","\n","random.shuffle(train_sample)\n","\n","true_pos = 0\n","total = 0\n","model.eval()\n","\n","from collections import defaultdict\n","\n","prediction_nums = {}\n","for emoji in tag_to_ix.keys():\n","  prediction_nums[emoji] = defaultdict(int)\n","\n","print(prediction_nums)\n","with torch.no_grad():\n","  for sample in test_sample:\n","    sentence = sample['words']\n","    # print(sentence, sample['emoji'])\n","    inputs = prepare_sequence(sentence, word_to_ix).to(device)\n","    tag_scores = model(inputs)\n","    # print(tag_scores)\n","    # print(np.argsort(-tag_scores.cpu().numpy()))\n","    \n","    scores = torch.argmax(tag_scores, dim=1)\n","    # print(scores)\n","    predicted = ix_to_emoji[stats.mode(scores.cpu().numpy())[0][0]]\n","\n","    if predicted == sample['emoji']:\n","      prediction_nums[sample['emoji']]['true_pos'] += 1\n","\n","      for temp_emoji in prediction_nums.keys():\n","        if temp_emoji != predicted: prediction_nums[temp_emoji]['true_neg'] += 1\n","      true_pos += 1\n","    \n","    else:\n","      prediction_nums[predicted]['false_pos'] += 1\n","      prediction_nums[sample['emoji']]['false_neg'] += 1\n","\n","    total += 1\n","    # print(predicted, sample['emoji'])\n","    if i%10000 == 0:\n","      print(true_pos/total)\n","    i+=1\n","\n","print(prediction_nums)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IDBW1WKwBUjP"},"source":["for emoji in prediction_nums.keys():\n","  prediction_nums[emoji]['precision'] = (prediction_nums[emoji]['true_pos'])/(prediction_nums[emoji]['true_pos']+prediction_nums[emoji]['false_pos'])\n","  prediction_nums[emoji]['recall'] = (prediction_nums[emoji]['true_pos'])/(prediction_nums[emoji]['true_pos']+prediction_nums[emoji]['false_neg'])\n","  f1 = 2*((prediction_nums[emoji]['precision']*prediction_nums[emoji]['recall'])/(prediction_nums[emoji]['precision']+prediction_nums[emoji]['recall']))\n","  print(emoji, \"prec:\", prediction_nums[emoji]['precision'], \"recall:\", prediction_nums[emoji]['recall'])\n","  print(\"f1:\", f1)"],"execution_count":null,"outputs":[]}]}